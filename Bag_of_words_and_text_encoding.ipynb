{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag-of-words and text encoding.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNmj4I5zmOb3GkcaGC8pV7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daivar/Deep_Learning_Models/blob/main/Bag_of_words_and_text_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzLp-s1EwGqz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "mxTpHgKkwUV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "COAN19TCwWvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "Xob0MjgAyUyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_data[0]))"
      ],
      "metadata": {
        "id": "nzux1L7-ycqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0]\n"
      ],
      "metadata": {
        "id": "KuijohUOyjcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[1]"
      ],
      "metadata": {
        "id": "yMhmPvfpzHbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(list(train_data[1])))"
      ],
      "metadata": {
        "id": "rwQdNANTyrgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing import sequence\n",
        "max([max(sequence) for sequence in train_data])"
      ],
      "metadata": {
        "id": "66EmpXzjzU-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "metadata": {
        "id": "m07-7koPzkLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_review"
      ],
      "metadata": {
        "id": "DsJG65u9ztCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "uYXpz-BS0LiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(x_train[0]))\n",
        "print(list(x_train[1]))\n",
        "print(len(list(x_train[1])))"
      ],
      "metadata": {
        "id": "0eBGFsf805ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "print(y_test[0])\n",
        "print(y_test[1])\n",
        "print(y_test[2])"
      ],
      "metadata": {
        "id": "VHeRFPQ11ph-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "NwIEXxAl1vhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y54HAOYr5f57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pX_0NoeQ5zUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])"
      ],
      "metadata": {
        "id": "-1aDyTOb51fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "cV49FYMX56bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "QxRg3Ggf5-gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "g_VyA3d36UrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6z428Ezt6Y-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### overfitinantis modelis, padetų dropautas, LR ratas, epocų apribojimas, "
      ],
      "metadata": {
        "id": "ewPh0x2z62RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['binary_accuracy']\n",
        "val_acc_values = history_dict['val_binary_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dd1e7res6ppN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "W3jk3m2S7yba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch sizas milžiniškas, nepasiekime ACC, mažinti iki 64"
      ],
      "metadata": {
        "id": "--mdKNEJ74SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "VY9jeSCr77Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(8, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(8, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "scores = cross_val_score(KerasClassifier(build_fn=lambda: model, epochs=1, batch_size=10, verbose=1), x_test, y_test, cv=4)"
      ],
      "metadata": {
        "id": "Nl_O7KL3AL8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "print('Maximum score in cross valuation is: %0.2f' % (scores.max()))"
      ],
      "metadata": {
        "id": "BF7y2J7zAZsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Against a Random Forest"
      ],
      "metadata": {
        "id": "QemMwTJaAgD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "YnB7FtP2AhnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "forest_model = RandomForestClassifier(max_depth=50, random_state=1)\n",
        "# forest_model.fit(x_train, y_train)\n",
        "# preds = forest_model.predict(x_test)\n",
        "# preds"
      ],
      "metadata": {
        "id": "lGR_P6fSAwPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# scores = cross_val_score(RandomForestClassifier(max_depth=50, random_state=1), x_test, y_test, cv=4)\n",
        "scores = cross_val_score(forest_model, x_test, y_test, cv=4)\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "print('Maximum score in cross valuation is: %0.2f'%(scores.max()))"
      ],
      "metadata": {
        "id": "zG_5L2MRAzEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
        "results = confusion_matrix(y_test, forest_model.predict(x_test).round())\n",
        "results\n",
        "\n",
        "# Deep learning confusion matrixs\n",
        "# array([[11318,  1182],\n",
        "#       [  411, 12089]])\n",
        "\n",
        "# array([[11560,   940],\n",
        "#       [  582, 11918]])"
      ],
      "metadata": {
        "id": "nl1aFJtgA3Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(results)\n",
        "\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QrLwq-97A635"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "word_index = imdb.get_word_index()"
      ],
      "metadata": {
        "id": "t6LcqDIuGmYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_encoded_to_string(data):\n",
        "    review = []\n",
        "    for given_value in data:\n",
        "        for word, value in word_index.items():\n",
        "            if value + 3 == given_value:\n",
        "                review.append(word)\n",
        "    return ' '.join(review)\n",
        "\n",
        "def convert_string_to_encoded(data):\n",
        "    review = []\n",
        "    for given_word in data.split():\n",
        "        for word, value in word_index.items():\n",
        "            if word == given_word:\n",
        "                review.append(value + 3)\n",
        "    return review\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "model.fit(x_train, to_categorical(y_train), epochs=4, batch_size=512,\n",
        "          validation_data=(x_test, to_categorical(y_test)))\n",
        "\n",
        "model.predict(x_test)\n",
        "\n",
        "custom_review1 = ('this is the best movie i have ever seen in my life '\n",
        "                  'amazing roles played by the characters this totally '\n",
        "                  'reminds me of the actual novel which is a must read '\n",
        "                  'as well')\n",
        "\n",
        "custom_review2 = ('this film was a garbage the story was meaningless '\n",
        "                  'and felt bad it was so boring that i felt i was '\n",
        "                  'being forced to watch this utter useless show')\n",
        "\n",
        "custom_review3 = ('the movie was fantastic, extraordinary every moment'\n",
        "                  'of the movie was worth it. it was very enjoyable')\n",
        "\n",
        "custom_reviews = (custom_review1, custom_review2, custom_review3)\n",
        "encoded_custom_reviews = np.array([convert_string_to_encoded(review) for review in custom_reviews])\n",
        "vectorized_custom_reviews = vectorize_sequences(encoded_custom_reviews)\n",
        "\n",
        "prediction = model.predict(vectorized_custom_reviews)\n",
        "print(prediction)\n",
        "\n",
        "\n",
        "# This model gives the output of the 1st layer of the\n",
        "# original model for some given data\n",
        "\n",
        "# micro_model = models.Sequential()\n",
        "# micro_model.add(model.layers[0])\n",
        "# intermediate_prediction = micro_model.predict(vectorized_custom_reviews)\n",
        "# print(intermediate_prediction)"
      ],
      "metadata": {
        "id": "myLy7HXaGr54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}